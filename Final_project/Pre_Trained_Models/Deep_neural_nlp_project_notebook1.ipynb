{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FZv-7cM63tiB",
        "PNB8qGu64j1L",
        "1DzJBOn05PFD",
        "d950DzbnInpu",
        "jXpoUms6c8pQ",
        "uVXIyPdcjpA2",
        "-KvTAd1axeFs",
        "RRY48qsaaFG1",
        "GY3boRePGO-T",
        "74LYjeqOxHss"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuT_f9fS9Xl7",
        "outputId": "653738d8-b12a-4e1e-eca2-b7f6f7a0143c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxFyv2Vc0lfS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L, d_k, d_v = 4,8,8\n",
        "q = np.random.randn(L, d_k)\n",
        "k = np.random.randn(L, d_k)\n",
        "v = np.random.randn(L, d_v)"
      ],
      "metadata": {
        "id": "Og_8zKQ321Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(q, '\\n')\n",
        "print(k, '\\n')\n",
        "print(v, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCPs3g9V3KcW",
        "outputId": "22431c43-f2a4-4b1e-d64c-ce95ead4a265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.96758812 -0.40146975  2.18849141 -2.06052671 -1.15993592 -0.05396872\n",
            "   0.19751109 -0.80567312]\n",
            " [-1.15515322  1.15279764 -0.62522654 -0.54217194 -1.19944445  0.37086141\n",
            "  -0.73540482 -0.73023423]\n",
            " [ 0.51305653 -0.90361321 -1.28030804  0.10329917  0.47429372 -0.54157\n",
            "  -0.17756604  0.8624922 ]\n",
            " [ 0.39749731 -1.49535567 -0.78455581 -1.17296651  2.03817588 -0.34945947\n",
            "   1.55562468  2.37512176]] \n",
            "\n",
            "[[-1.24458168  1.21412414 -0.62577755 -0.85658199  0.50415185 -0.61579608\n",
            "   0.20804076  0.82455895]\n",
            " [-1.65576569  0.51017098  0.78565181  0.39557238 -1.11090295  0.35500472\n",
            "   1.32848951 -1.60721435]\n",
            " [-0.91639866  0.05486944 -0.06290943  0.12033662 -0.73808194  1.76629215\n",
            "   0.23217683  0.20803954]\n",
            " [-2.48537524  1.98691324  0.36446368  0.96925111 -0.3866674  -1.06743508\n",
            "  -0.87364681 -0.3305322 ]] \n",
            "\n",
            "[[ 0.95096576 -1.90498352  0.62781283 -0.26736013 -0.29318037  0.20819307\n",
            "  -1.0450522  -0.42213853]\n",
            " [-1.72952972 -0.18825185 -1.14388353  0.23565835 -0.75810006 -0.2512609\n",
            "  -0.098343   -0.85321607]\n",
            " [ 0.15009768  0.46603061 -2.31453159  1.22620733 -0.64736723  0.16127519\n",
            "   1.47226738 -1.45508088]\n",
            " [ 0.17808605  0.46341608  1.71108367  0.57580887  0.23907297 -0.89590027\n",
            "   1.26401385 -0.03291496]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# self attention"
      ],
      "metadata": {
        "id": "FZv-7cM63tiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.matmul(q,k.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWKNtHYH3VRR",
        "outputId": "3be8ce32-be7d-4f33-8668-3a2c6786d9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.47096003,  1.92408525, -0.65530959, -3.80218381],\n",
              "       [ 2.10479696,  3.45589764,  2.31360005,  5.35989059],\n",
              "       [ 0.22391188, -4.61677131, -1.59520365, -3.17229652],\n",
              "       [ 2.71024127, -6.64040441, -1.80439928, -7.94110479]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled = np.matmul(q,k.T) / math.sqrt(d_k)\n",
        "q.var(), k.var(), scaled.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO5N-YFB3ybX",
        "outputId": "f79a3450-9bb0-4742-b866-05713f13cea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2073969672027554, 1.022938702053252, 1.668318358729655)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nygtbxgl4Gsk",
        "outputId": "e629b1f0-2b6c-4ae4-f662-5ed2501bff79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.8736163 ,  0.68026687, -0.23168693, -1.34427498],\n",
              "       [ 0.7441581 ,  1.22184433,  0.81798114,  1.89500749],\n",
              "       [ 0.0791648 , -1.63227515, -0.56398966, -1.12157619],\n",
              "       [ 0.95821499, -2.34773749, -0.63795148, -2.80760452]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# masking"
      ],
      "metadata": {
        "id": "PNB8qGu64j1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.tril(np.ones( (L,L) ))\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uOdqwlg4Noc",
        "outputId": "7f5054c4-2005-4a3d-b8d1-3a1195c8ed31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUN6UOzQ4nx6",
        "outputId": "317f436c-778d-40ff-dc1e-d0f65f2c8af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled + mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMH5WrRT5CN3",
        "outputId": "babfc222-b868-4366-c134-871b251acc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.8736163 ,        -inf,        -inf,        -inf],\n",
              "       [ 0.7441581 ,  1.22184433,        -inf,        -inf],\n",
              "       [ 0.0791648 , -1.63227515, -0.56398966,        -inf],\n",
              "       [ 0.95821499, -2.34773749, -0.63795148, -2.80760452]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# softmax"
      ],
      "metadata": {
        "id": "1DzJBOn05PFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T/ np.sum(np.exp(x), axis=-1)).T"
      ],
      "metadata": {
        "id": "1uf0tuO45HFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = softmax(scaled + mask)"
      ],
      "metadata": {
        "id": "r5q3UlKt5bk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dWi6Cja5iUN",
        "outputId": "5eda13d7-3051-46c0-c2cb-dd12a02b8d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.38279864, 0.61720136, 0.        , 0.        ],\n",
              "       [0.58608496, 0.10585019, 0.30806485, 0.        ],\n",
              "       [0.79208869, 0.02904136, 0.16053418, 0.01833577]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_v = np.matmul(attention,v)\n",
        "new_v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tkDTPQT5lAq",
        "outputId": "9918bc75-c77f-44cb-f5e4-0f7fe5ae3dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.95096576, -1.90498352,  0.62781283, -0.26736013, -0.29318037,\n",
              "         0.20819307, -1.0450522 , -0.42213853],\n",
              "       [-0.7034397 , -0.84541439, -0.46568058,  0.04310356, -0.58012944,\n",
              "        -0.07538255, -0.46074199, -0.68820018],\n",
              "       [ 0.4205155 , -0.99284104, -0.46615445,  0.2460001 , -0.45150473,\n",
              "         0.14510603, -0.16934518, -0.7859814 ],\n",
              "       [ 0.73038249, -1.43107206,  0.12387611,  0.00247699, -0.35378209,\n",
              "         0.16707358, -0.57110413, -0.59334345]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWXJSMZK57_O",
        "outputId": "4037a0e7-77a4-4bc2-8f81-be67641a4734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.95096576, -1.90498352,  0.62781283, -0.26736013, -0.29318037,\n",
              "         0.20819307, -1.0450522 , -0.42213853],\n",
              "       [-1.72952972, -0.18825185, -1.14388353,  0.23565835, -0.75810006,\n",
              "        -0.2512609 , -0.098343  , -0.85321607],\n",
              "       [ 0.15009768,  0.46603061, -2.31453159,  1.22620733, -0.64736723,\n",
              "         0.16127519,  1.47226738, -1.45508088],\n",
              "       [ 0.17808605,  0.46341608,  1.71108367,  0.57580887,  0.23907297,\n",
              "        -0.89590027,  1.26401385, -0.03291496]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q,k,v,mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = np.matmul(q,k.T) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention,v)\n",
        "  return out, attention"
      ],
      "metadata": {
        "id": "NSF1E9Vl5_E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values, attention = scaled_dot_product_attention(q,k,v,mask=mask)\n",
        "print('Q\\n', q)\n",
        "print('K\\n', k)\n",
        "print('V\\n', v)\n",
        "print('New V\\n', values)\n",
        "print('Attention\\n', attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3pg7vWs6wxd",
        "outputId": "07c21cda-8a5b-42c7-e690-591c30fe35c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[ 0.96758812 -0.40146975  2.18849141 -2.06052671 -1.15993592 -0.05396872\n",
            "   0.19751109 -0.80567312]\n",
            " [-1.15515322  1.15279764 -0.62522654 -0.54217194 -1.19944445  0.37086141\n",
            "  -0.73540482 -0.73023423]\n",
            " [ 0.51305653 -0.90361321 -1.28030804  0.10329917  0.47429372 -0.54157\n",
            "  -0.17756604  0.8624922 ]\n",
            " [ 0.39749731 -1.49535567 -0.78455581 -1.17296651  2.03817588 -0.34945947\n",
            "   1.55562468  2.37512176]]\n",
            "K\n",
            " [[-1.24458168  1.21412414 -0.62577755 -0.85658199  0.50415185 -0.61579608\n",
            "   0.20804076  0.82455895]\n",
            " [-1.65576569  0.51017098  0.78565181  0.39557238 -1.11090295  0.35500472\n",
            "   1.32848951 -1.60721435]\n",
            " [-0.91639866  0.05486944 -0.06290943  0.12033662 -0.73808194  1.76629215\n",
            "   0.23217683  0.20803954]\n",
            " [-2.48537524  1.98691324  0.36446368  0.96925111 -0.3866674  -1.06743508\n",
            "  -0.87364681 -0.3305322 ]]\n",
            "V\n",
            " [[ 0.95096576 -1.90498352  0.62781283 -0.26736013 -0.29318037  0.20819307\n",
            "  -1.0450522  -0.42213853]\n",
            " [-1.72952972 -0.18825185 -1.14388353  0.23565835 -0.75810006 -0.2512609\n",
            "  -0.098343   -0.85321607]\n",
            " [ 0.15009768  0.46603061 -2.31453159  1.22620733 -0.64736723  0.16127519\n",
            "   1.47226738 -1.45508088]\n",
            " [ 0.17808605  0.46341608  1.71108367  0.57580887  0.23907297 -0.89590027\n",
            "   1.26401385 -0.03291496]]\n",
            "New V\n",
            " [[ 0.95096576 -1.90498352  0.62781283 -0.26736013 -0.29318037  0.20819307\n",
            "  -1.0450522  -0.42213853]\n",
            " [-0.7034397  -0.84541439 -0.46568058  0.04310356 -0.58012944 -0.07538255\n",
            "  -0.46074199 -0.68820018]\n",
            " [ 0.4205155  -0.99284104 -0.46615445  0.2460001  -0.45150473  0.14510603\n",
            "  -0.16934518 -0.7859814 ]\n",
            " [ 0.73038249 -1.43107206  0.12387611  0.00247699 -0.35378209  0.16707358\n",
            "  -0.57110413 -0.59334345]]\n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.38279864 0.61720136 0.         0.        ]\n",
            " [0.58608496 0.10585019 0.30806485 0.        ]\n",
            " [0.79208869 0.02904136 0.16053418 0.01833577]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# multi-head attention"
      ],
      "metadata": {
        "id": "d950DzbnInpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 4\n",
        "batch_size = 1\n",
        "input_dim = 512\n",
        "d_model = 512\n",
        "x = tf.experimental.numpy.random.randn(batch_size, sequence_length, input_dim)"
      ],
      "metadata": {
        "id": "_-zlzK0ZIuiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niiSTyFJJRWc",
        "outputId": "e87967eb-ffeb-40ec-8dd6-31c8aef2418d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 4, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qkv_layer = tf.keras.layers.Dense(units=3*d_model, input_shape=[input_dim,1], activation=None)"
      ],
      "metadata": {
        "id": "O0az4v3jJcey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv = qkv_layer(x)"
      ],
      "metadata": {
        "id": "wc1DWqS7KrBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEAphzCOK3aC",
        "outputId": "6d1b8297-8e5a-4707-fbe2-2c081ae9cdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4, 1536), dtype=float32, numpy=\n",
              "array([[[ 0.02269682,  0.13334104,  0.30694684, ...,  0.70302117,\n",
              "         -0.35789302, -0.08504933],\n",
              "        [-0.25561184,  0.51563776,  0.9664403 , ...,  1.0077868 ,\n",
              "          0.72439253, -0.5487225 ],\n",
              "        [-0.99646777,  0.37771612, -0.608686  , ...,  0.01303208,\n",
              "          1.6508543 ,  0.17800407],\n",
              "        [ 0.30492818,  0.13314196, -0.30292076, ..., -0.63025534,\n",
              "          0.7847659 ,  1.0763559 ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qkv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkUrbqNbK4pg",
        "outputId": "b70b4c9c-0590-4d92-9c95-e215eccf0558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 4, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val = tf.histogram_fixed_width(qkv, [-3,3], nbins=200)\n",
        "x_val = np.arange(-1,1,0.01)*3\n",
        "plt.bar(x_val, y_val, align='center')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "CL5ePltTK_w9",
        "outputId": "58edf6a6-b2f1-4ce2-afa9-936ee811cc18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 200 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 8\n",
        "head_dim = d_model // num_heads\n",
        "qkv = tf.reshape(qkv, [batch_size, sequence_length, num_heads, 3*head_dim])"
      ],
      "metadata": {
        "id": "bonpxrfcLw6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv.shape"
      ],
      "metadata": {
        "id": "HjNVJoqbPpCv",
        "outputId": "eca96494-8105-4d49-f1a9-027fed598604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 4, 8, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])"
      ],
      "metadata": {
        "id": "1dCZEOtHOPZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJTX56D7Rwio",
        "outputId": "1bc68c98-7417-44ea-ae8d-977f85dac170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 8, 4, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q,k,v = tf.split(qkv, 3, axis=-1)\n",
        "q.shape, k.shape, v.shape"
      ],
      "metadata": {
        "id": "X1P-pvH-OdX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b9ccb4-5bef-4c47-c0ca-6505676d3f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 8, 4, 64]),\n",
              " TensorShape([1, 8, 4, 64]),\n",
              " TensorShape([1, 8, 4, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# self-attention for multi heads"
      ],
      "metadata": {
        "id": "jXpoUms6c8pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = q.shape[-1]\n",
        "scaled = tf.linalg.matmul(q, tf.transpose(k, perm=[0,1,3,2]))/math.sqrt(d_k)\n",
        "scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhSr6W3DSEt7",
        "outputId": "db4fe0cb-3edf-4071-8411-d4df9c3be426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = tf.experimental.numpy.triu(\n",
        "    tf.fill(scaled.shape, -float('inf')), k=1\n",
        ")\n",
        "mask[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCrz0UXPeDSx",
        "outputId": "44c30571-d988-4036-d8a2-75f0198ecc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(scaled+mask)[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrNLVPIzfMDA",
        "outputId": "af022d87-c81e-49ce-c1f5-dd6af6471768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
              "array([[ 0.03382403,        -inf,        -inf,        -inf],\n",
              "       [ 0.40852624,  0.19772756,        -inf,        -inf],\n",
              "       [ 0.44695693, -0.04619832,  0.71070087,        -inf],\n",
              "       [ 0.31117177, -0.09555028,  0.09923264, -0.69150394]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled += mask"
      ],
      "metadata": {
        "id": "5jvU6Fxro_Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = tf.keras.layers.Softmax(\n",
        "    axis=-1)(scaled)"
      ],
      "metadata": {
        "id": "nwZCw4X1pU_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5mp872LprA9",
        "outputId": "94c98eef-a4f9-4558-8b76-5703f415bd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSz1U_w5pww-",
        "outputId": "b0abd557-fcd0-4a72-f664-58ca6611d137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
              "array([[0.99999994, 0.        , 0.        , 0.        ],\n",
              "       [0.5800391 , 0.41996104, 0.        , 0.        ],\n",
              "       [0.4152224 , 0.33018377, 0.25459385, 0.        ],\n",
              "       [0.18456772, 0.45852146, 0.1584284 , 0.19848241]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values = tf.linalg.matmul(attention,v)"
      ],
      "metadata": {
        "id": "VEXNgtIgp02G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDUPKYy5p9XB",
        "outputId": "b08e8012-2e44-464b-b330-cbda7d9e9432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8, 4, 64), dtype=float32, numpy=\n",
              "array([[[[ 0.07677756, -0.8028991 ,  0.416999  , ...,  1.1993831 ,\n",
              "           0.3989647 ,  0.03577034],\n",
              "         [ 0.06373545, -0.38767254,  0.16199876, ...,  0.33009982,\n",
              "           0.4581686 , -0.616297  ],\n",
              "         [-0.18731739, -0.15258214,  0.08465487, ...,  0.4272296 ,\n",
              "           0.44975096, -0.30410314],\n",
              "         [-0.43370602,  0.07009131,  0.02384048, ..., -0.30419996,\n",
              "           0.30547428, -0.5999064 ]],\n",
              "\n",
              "        [[ 0.10872214,  0.13181518, -0.38858643, ..., -1.1281129 ,\n",
              "          -0.25746676,  0.47692582],\n",
              "         [-0.07372171,  0.43821526, -0.59366524, ..., -0.6563602 ,\n",
              "           0.1256997 ,  0.31178916],\n",
              "         [ 0.0382386 ,  0.61404455, -0.83080024, ..., -0.46953416,\n",
              "          -0.07604705, -0.29038656],\n",
              "         [ 0.03930473,  0.46189433, -0.670296  , ..., -0.3028288 ,\n",
              "          -0.07931034, -0.02533519]],\n",
              "\n",
              "        [[ 1.0198421 , -0.68251276,  1.0378771 , ...,  0.15581325,\n",
              "           0.79219764, -0.05340451],\n",
              "         [ 0.35571003, -0.5535912 ,  0.4367754 , ..., -0.06388928,\n",
              "           0.5681986 ,  0.20079392],\n",
              "         [ 0.1072407 , -0.35770112,  0.07021865, ...,  0.3212813 ,\n",
              "           0.37081134, -0.31671953],\n",
              "         [-0.21415435, -0.3151936 , -0.06190461, ...,  0.1167496 ,\n",
              "           0.22804758, -0.14963542]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.54468584,  0.00941857,  0.7623219 , ...,  0.5597005 ,\n",
              "           0.7897945 , -1.4001558 ],\n",
              "         [-0.10438018, -0.21553054, -0.29293355, ...,  0.69028455,\n",
              "           0.5263848 , -0.5397242 ],\n",
              "         [-0.09847163, -0.14990616, -0.24940532, ...,  0.18016368,\n",
              "           0.80073094, -0.689996  ],\n",
              "         [-0.4990133 , -0.10577632,  0.19110033, ...,  0.26034033,\n",
              "           0.6060009 , -0.8057999 ]],\n",
              "\n",
              "        [[ 0.60531646,  0.40599123,  0.79129845, ..., -0.5701504 ,\n",
              "           0.1446254 , -1.7862813 ],\n",
              "         [ 1.0300887 , -0.18335697,  0.09668788, ...,  0.38838   ,\n",
              "           0.621353  , -0.51626027],\n",
              "         [ 0.49967584, -0.06318986,  0.46072704, ...,  0.23898576,\n",
              "           0.02124566, -0.33126894],\n",
              "         [ 0.49458236,  0.01787864,  0.03123096, ...,  0.4293031 ,\n",
              "           0.21017812, -0.14309525]],\n",
              "\n",
              "        [[ 0.5778187 ,  0.5553605 , -0.37023976, ...,  0.7030211 ,\n",
              "          -0.357893  , -0.08504932],\n",
              "         [ 0.3906857 , -1.0396978 ,  0.8304933 , ...,  0.9183057 ,\n",
              "           0.406627  , -0.41258526],\n",
              "         [ 0.40388688, -0.885581  ,  0.3384149 , ...,  0.58778936,\n",
              "           0.95195484, -0.21302596],\n",
              "         [-0.02346918, -0.30762172, -0.17195974, ...,  0.147638  ,\n",
              "           0.7294823 ,  0.27754277]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values = tf.reshape(values, [batch_size,sequence_length, num_heads*head_dim])\n",
        "values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETi0zRqLp-UP",
        "outputId": "699fe8fc-6f40-452c-fac2-5250b6ec023d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4, 512), dtype=float32, numpy=\n",
              "array([[[ 0.07677756, -0.8028991 ,  0.416999  , ..., -0.3028288 ,\n",
              "         -0.07931034, -0.02533519],\n",
              "        [ 1.0198421 , -0.68251276,  1.0378771 , ..., -0.15240796,\n",
              "         -0.07409684,  0.01128155],\n",
              "        [-0.08656189,  0.06662884, -1.5984715 , ...,  0.26034033,\n",
              "          0.6060009 , -0.8057999 ],\n",
              "        [ 0.60531646,  0.40599123,  0.79129845, ...,  0.147638  ,\n",
              "          0.7294823 ,  0.27754277]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layer = tf.keras.layers.Dense(d_model,input_shape=[d_model,1], activation=None)"
      ],
      "metadata": {
        "id": "8R6VPXu8qYmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = linear_layer(values)"
      ],
      "metadata": {
        "id": "6T3VbJxTqshk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WUzMnc9qzpW",
        "outputId": "3c1af278-0e03-4209-c8b9-ad2dc01ec74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 4, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q,k,v,mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = tf.linalg.matmul(q,tf.transpose(k, perm=[0,1,3,2])) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "\n",
        "  attention = tf.nn.softmax(scaled, axis=-1)\n",
        "  out = tf.linalg.matmul(attention,v)\n",
        "  return out, attention\n",
        "\n",
        "class MultiheadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, input_dim, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.qkv_layer = tf.keras.layers.Dense(units=3*d_model, input_shape=[input_dim,1], activation=None)\n",
        "    # self.qkv_layer = self.add_weight(shape=[input_dim,3*d_model], trainable=True)\n",
        "    self.linear_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "    # self.linear_layer = self.add_weight(shape=[d_model,d_model], trainable=True)\n",
        "\n",
        "  def call(self,x,mask=None):\n",
        "    batch_size, sequence_length, input_dim = x.shape\n",
        "    qkv = self.qkv_layer(x)\n",
        "    qkv = tf.reshape(qkv, [batch_size, sequence_length, self.num_heads, 3*self.head_dim])\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q,k,v = tf.split(qkv, 3, axis=-1)\n",
        "    values, attention = scaled_dot_product_attention(q,k,v,mask)\n",
        "    values = tf.reshape(values, [batch_size, sequence_length, self.num_heads*self.head_dim])\n",
        "    out = self.linear_layer(values)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "7eYq-fx6q1AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 1024\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "batch_size = 30\n",
        "sequence_length = 5\n",
        "x = tf.experimental.numpy.random.randn(batch_size, sequence_length, input_dim)\n",
        "model = MultiheadAttention(input_dim,d_model, num_heads)\n",
        "out = model(x)"
      ],
      "metadata": {
        "id": "h8DdGPJTZBSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufrDc4MtfvLI",
        "outputId": "93402c82-5390-4614-d089-fac364578f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([30, 5, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional encoding"
      ],
      "metadata": {
        "id": "uVXIyPdcjpA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, max_sequence_length):\n",
        "    super().__init__()\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def call(self):\n",
        "    even_i = tf.experimental.numpy.arange(0, stop=self.d_model, step=2, dtype=float)\n",
        "    denominator = tf.math.pow(10000, even_i/self.d_model)\n",
        "    position = tf.reshape(tf.experimental.numpy.arange(0,stop=self.max_sequence_length, dtype=denominator.dtype), [self.max_sequence_length,1])\n",
        "    even_PE = tf.math.sin(position/denominator)\n",
        "    odd_PE = tf.math.cos(position/denominator)\n",
        "    stacked = tf.stack([even_PE,odd_PE], axis=2)\n",
        "    PE = tf.keras.layers.Flatten()(stacked)\n",
        "    return PE"
      ],
      "metadata": {
        "id": "Nb079qSlf-oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def call(self, d_model, max_sequence_length):\n",
        "    d_model = tf.cast(d_model, tf.float64)\n",
        "    max_sequence_length = tf.cast(max_sequence_length, tf.float64)\n",
        "    even_i = tf.experimental.numpy.arange(0, stop=d_model, step=2, dtype=float)\n",
        "    denominator = tf.math.pow(tf.cast(10000, tf.float64), even_i/d_model)\n",
        "    position = tf.reshape(tf.experimental.numpy.arange(0,stop=max_sequence_length, dtype=denominator.dtype), [max_sequence_length,1])\n",
        "    even_PE = tf.math.sin(position/denominator)\n",
        "    odd_PE = tf.math.cos(position/denominator)\n",
        "    stacked = tf.stack([even_PE,odd_PE], axis=2)\n",
        "    PE = tf.keras.layers.Flatten()(stacked)\n",
        "    return PE"
      ],
      "metadata": {
        "id": "MQfoluEOvK4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding()(d_model=6, max_sequence_length=10)\n",
        "pe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU2R76RWl20f",
        "outputId": "7892836f-5d04-4f51-ef09-83b5e959c786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 6), dtype=float32, numpy=\n",
              "array([[ 0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
              "         1.        ],\n",
              "       [ 0.84147096,  0.5403023 ,  0.04639922,  0.998923  ,  0.00215443,\n",
              "         0.9999977 ],\n",
              "       [ 0.9092974 , -0.41614684,  0.0926985 ,  0.9956942 ,  0.00430886,\n",
              "         0.9999907 ],\n",
              "       [ 0.14112   , -0.9899925 ,  0.1387981 ,  0.9903207 ,  0.00646326,\n",
              "         0.99997914],\n",
              "       [-0.7568025 , -0.6536436 ,  0.18459873,  0.98281395,  0.00861763,\n",
              "         0.99996287],\n",
              "       [-0.9589243 ,  0.2836622 ,  0.23000172,  0.97319025,  0.01077196,\n",
              "         0.999942  ],\n",
              "       [-0.2794155 ,  0.96017027,  0.27490926,  0.9614702 ,  0.01292625,\n",
              "         0.99991643],\n",
              "       [ 0.6569866 ,  0.75390226,  0.31922466,  0.94767904,  0.01508047,\n",
              "         0.9998863 ],\n",
              "       [ 0.98935825, -0.14550003,  0.36285242,  0.9318466 ,  0.01723462,\n",
              "         0.99985147],\n",
              "       [ 0.4121185 , -0.91113025,  0.40569857,  0.91400695,  0.0193887 ,\n",
              "         0.999812  ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer Normalization"
      ],
      "metadata": {
        "id": "-KvTAd1axeFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "  def __init__(self, parameters_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters_shape = parameters_shape\n",
        "    self.eps = eps\n",
        "    self.gamma = self.add_weight(shape=parameters_shape, initializer=tf.keras.initializers.Constant(value=1),trainable=True)\n",
        "    self.beta = self.add_weight(shape=parameters_shape, initializer=tf.keras.initializers.Constant(value=0),trainable=True)\n",
        "\n",
        "  def call(self, input):\n",
        "    dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
        "    mean = tf.math.reduce_mean(input, axis = dims, keepdims=True)\n",
        "    var = tf.math.reduce_mean(((input - mean) ** 2), axis = dims, keepdims=True)\n",
        "    std = tf.math.sqrt((var + self.eps))\n",
        "    y = (input - mean) / std\n",
        "    out = self.gamma * y + self.beta\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "hCyltNxrmA_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.experimental.numpy.random.randn(batch_size, sequence_length, 512)\n",
        "layer = LayerNormalization(parameters_shape=[512])\n",
        "layer(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRFdkFjYriv9",
        "outputId": "a61325fb-46b9-453b-f2a4-e597e0b56884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30, 5, 512), dtype=float32, numpy=\n",
              "array([[[-0.59065086, -0.55234396,  0.53714997, ...,  0.0430456 ,\n",
              "         -1.6428852 , -0.85690325],\n",
              "        [ 1.4635557 ,  0.07678024, -1.5657271 , ..., -0.88312155,\n",
              "         -0.5795958 ,  0.7273063 ],\n",
              "        [ 0.30792814,  0.75903445,  0.16480885, ...,  0.300725  ,\n",
              "          0.46388632,  1.1672838 ],\n",
              "        [-0.8334757 ,  1.8001858 , -0.43668064, ..., -0.20089668,\n",
              "          0.9106056 , -0.67561096],\n",
              "        [ 0.29541305,  0.22486064,  0.21803203, ..., -1.1799544 ,\n",
              "         -0.30272996, -3.1579776 ]],\n",
              "\n",
              "       [[ 0.83914506,  0.7880799 , -1.685045  , ..., -0.1847178 ,\n",
              "         -1.6700857 , -1.2737597 ],\n",
              "        [-0.70374745, -0.13768704,  0.41842818, ...,  1.5532963 ,\n",
              "          0.11830629, -0.22298202],\n",
              "        [-1.5157161 , -0.7038043 ,  0.13204554, ..., -1.2613208 ,\n",
              "         -0.04532935,  0.79891884],\n",
              "        [-0.44711804, -0.01101441, -1.5050968 , ...,  0.2825658 ,\n",
              "         -0.93766195, -1.3327582 ],\n",
              "        [-0.3140302 , -1.7224556 ,  0.23073818, ...,  0.6709445 ,\n",
              "          0.814622  ,  1.454816  ]],\n",
              "\n",
              "       [[ 0.6210121 , -1.1514153 ,  0.14579248, ..., -0.07559964,\n",
              "          1.0100082 , -1.6481626 ],\n",
              "        [-0.56430197, -0.8120783 , -0.5045761 , ..., -1.6343242 ,\n",
              "          0.34641555,  0.29575178],\n",
              "        [-1.2356703 ,  1.5326174 , -0.10194451, ..., -1.2341584 ,\n",
              "          0.69875205, -0.0430682 ],\n",
              "        [ 0.12022474, -0.532972  , -0.768791  , ..., -0.01116837,\n",
              "          1.492319  ,  1.2164494 ],\n",
              "        [-1.755458  ,  0.6538417 ,  2.579121  , ..., -0.07130664,\n",
              "          1.5637287 , -1.7393248 ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-1.0521206 , -0.60220903,  0.10356315, ...,  0.23581032,\n",
              "         -0.67585593, -1.8054651 ],\n",
              "        [ 0.4104148 ,  0.74338704,  0.86157787, ..., -0.5183889 ,\n",
              "          1.2449428 ,  0.27687246],\n",
              "        [ 0.79367036,  1.0525475 , -0.84875906, ...,  0.84786636,\n",
              "         -1.6183654 , -0.33397475],\n",
              "        [-0.49341246, -2.0112686 , -1.0636734 , ..., -0.8895597 ,\n",
              "          1.3709173 , -1.1410991 ],\n",
              "        [-0.18264326,  1.5217572 ,  0.3715282 , ...,  0.884329  ,\n",
              "         -0.9591427 ,  0.88646245]],\n",
              "\n",
              "       [[-0.8320273 ,  0.02753882, -1.0341971 , ...,  3.2645173 ,\n",
              "         -2.1047897 ,  0.01176104],\n",
              "        [-1.076893  , -0.29921517,  0.7586501 , ..., -0.92115235,\n",
              "         -0.7009381 ,  0.10070933],\n",
              "        [ 1.1622664 , -0.16366225, -0.94364107, ..., -0.06216097,\n",
              "         -0.68350285,  0.9154246 ],\n",
              "        [-0.52715254, -0.9611005 , -0.6173245 , ..., -0.42521468,\n",
              "          0.37996703, -1.1030903 ],\n",
              "        [-0.44284037,  1.3544356 ,  1.1970179 , ...,  0.8209959 ,\n",
              "         -0.03130467, -0.9520978 ]],\n",
              "\n",
              "       [[-0.18287246, -0.95189315, -1.1065303 , ...,  1.0061356 ,\n",
              "          0.37825894, -0.08540849],\n",
              "        [ 1.9531732 ,  1.7863212 , -0.08630495, ..., -0.17347187,\n",
              "          0.7218629 ,  0.729615  ],\n",
              "        [ 0.22076753, -0.07305393, -0.10254059, ...,  0.975335  ,\n",
              "         -0.83611315,  0.1229611 ],\n",
              "        [ 0.24390176, -1.1973604 , -0.4487946 , ...,  0.5100223 ,\n",
              "         -0.19766878, -0.44435835],\n",
              "        [-0.65551853,  0.09899767, -0.87020785, ..., -0.09695035,\n",
              "          0.53589153,  0.9638101 ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = MultiheadAttention(input_dim,d_model, num_heads)\n",
        "# print([var.name for var in layer.trainable])"
      ],
      "metadata": {
        "id": "fyHSO2lF1rn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer.trainable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZVEPNbO_36j",
        "outputId": "d93371a5-3feb-4053-80f0-ac917e2b38c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDAj1z_L166E",
        "outputId": "53623f07-0ac2-44c3-b2ab-fcd2584b8d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "RRY48qsaaFG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5"
      ],
      "metadata": {
        "id": "CUL1y6IQ2GGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q,k,v,mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = tf.linalg.matmul(q,tf.transpose(k, perm=[0,1,3,2])) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "\n",
        "  attention = tf.nn.softmax(scaled, axis=-1)\n",
        "  out = tf.linalg.matmul(attention,v)\n",
        "  return out, attention\n",
        "\n",
        "class MultiheadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.qkv_layer = tf.keras.layers.Dense(units=3*d_model, input_shape=[d_model,1], activation=None)\n",
        "    # self.qkv_layer = self.add_weight(shape=[input_dim,3*d_model], trainable=True)\n",
        "    self.linear_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "    # self.linear_layer = self.add_weight(shape=[d_model,d_model], trainable=True)\n",
        "\n",
        "  def call(self,x,mask=None):\n",
        "    batch_size, sequence_length, d_model = x.shape\n",
        "    qkv = self.qkv_layer(x)\n",
        "    qkv = tf.reshape(qkv, [batch_size, sequence_length, self.num_heads, 3*self.head_dim])\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q,k,v = tf.split(qkv, 3, axis=-1)\n",
        "    values, attention = scaled_dot_product_attention(q,k,v,mask)\n",
        "    values = tf.reshape(tf.transpose(values, perm=[0, 2, 1, 3]), [batch_size, sequence_length, self.num_heads*self.head_dim])\n",
        "    out = self.linear_layer(values)\n",
        "    return out\n",
        "\n",
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "  def __init__(self, parameters_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters_shape = parameters_shape\n",
        "    self.eps = eps\n",
        "    self.gamma = self.add_weight(shape=parameters_shape, initializer=tf.keras.initializers.Constant(value=1),trainable=True)\n",
        "    self.beta = self.add_weight(shape=parameters_shape, initializer=tf.keras.initializers.Constant(value=0),trainable=True)\n",
        "\n",
        "  def call(self, input):\n",
        "    dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
        "    mean = tf.math.reduce_mean(input, axis = dims, keepdims=True)\n",
        "    var = tf.math.reduce_mean(((input - mean) ** 2), axis = dims, keepdims=True)\n",
        "    std = tf.math.sqrt((var + self.eps))\n",
        "    y = (input - mean) / std\n",
        "    out = self.gamma * y + self.beta\n",
        "    return out\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def call(self, d_model, max_sequence_length):\n",
        "    d_model = tf.cast(d_model, tf.float64)\n",
        "    max_sequence_length = tf.cast(max_sequence_length, tf.float64)\n",
        "    even_i = tf.experimental.numpy.arange(0, stop=d_model, step=2, dtype=float)\n",
        "    denominator = tf.math.pow(tf.cast(10000, tf.float64), even_i/d_model)\n",
        "    position = tf.reshape(tf.experimental.numpy.arange(0,stop=max_sequence_length, dtype=denominator.dtype), [max_sequence_length,1])\n",
        "    even_PE = tf.math.sin(position/denominator)\n",
        "    odd_PE = tf.math.cos(position/denominator)\n",
        "    stacked = tf.stack([even_PE,odd_PE], axis=2)\n",
        "    PE = tf.keras.layers.Flatten()(stacked)\n",
        "    return PE\n",
        "\n",
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model, hidden, drop_prob):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.linear1 = tf.keras.layers.Dense(units=hidden, input_shape=[d_model,1], activation=None)\n",
        "    self.linear2 = tf.keras.layers.Dense(units=d_model, input_shape=[hidden,1], activation=None)\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "    self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.attention = MultiheadAttention(d_model, num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = tf.keras.layers.Dropout(drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model = d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "  def call(self, x, self_attention_mask):\n",
        "    residual_x = tf.identity(x)\n",
        "    x = self.attention(x, mask=None)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.norm1(x + tf.cast(residual_x, dtype=x.dtype))\n",
        "    residual_x = tf.identity(x)\n",
        "    x = self.ffn(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.norm2(x + tf.cast(residual_x, dtype=x.dtype))\n",
        "    return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, language_to_index, START_TOKEN,END_TOKEN, PADDING_TOKEN):\n",
        "    super().__init__()\n",
        "    # self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.layers = [\n",
        "        EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def call(self, x, self_attention_mask, start_token, end_token):\n",
        "    # x = self.sentence_embedding(x, start_token, end_token)\n",
        "    for layer in range(self.num_layers):\n",
        "      x = self.layers[layer](x, self_attention_mask)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ygZH3EfJgsKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.layers = [\n",
        "        DecoderLayer(d_model,ffn_hidden,num_heads,drop_prob)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def call(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "    # y = self.sentence_embedding(y, start_token, end_token)\n",
        "    for layer in range(self.num_layers):\n",
        "      y = self.layers[layer](x, y, self_attention_mask, cross_attention_mask)\n",
        "\n",
        "    return y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "jkyy8y-nM0g7",
        "outputId": "b6609592-c2ed-4927-db68-0fc358787e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    def call(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, [], 's', 'e', 'p')\n",
        "x = tf.experimental.numpy.random.randn(batch_size, max_sequence_length, d_model)\n",
        "out = encoder(x, None, 's', 'e')"
      ],
      "metadata": {
        "id": "6B42pM1apQUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uAOCe-Wb4lb",
        "outputId": "dbb8f9c7-cb28-4fce-ae70-b6bff4e7de3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([30, 200, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "GY3boRePGO-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5"
      ],
      "metadata": {
        "id": "94wKvgGfGGQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadCrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.kv_layer = tf.keras.layers.Dense(units=2*d_model, input_shape=[d_model,1], activation=None)\n",
        "    self.q_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "    self.linear_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "\n",
        "  def call(self, x, y, mask=None):\n",
        "    batch_size, sequence_length, d_model = x.shape\n",
        "    kv = self.kv_layer(x)\n",
        "    q = self.q_layer(x)\n",
        "    kv = tf.reshape(kv, [batch_size, sequence_length, self.num_heads, 2*self.head_dim])\n",
        "    q = tf.reshape(q, [batch_size, sequence_length, self.num_heads, self.head_dim])\n",
        "    kv = tf.transpose(kv, perm=[0, 2, 1, 3])\n",
        "    q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "    k,v = tf.split(kv, 2, axis=-1)\n",
        "    values, attention = scaled_dot_product_attention(q,k,v,mask)\n",
        "    values = tf.reshape(tf.transpose(values, perm=[0, 2, 1, 3]), [batch_size, sequence_length, d_model])\n",
        "    out = self.linear_layer(values)\n",
        "    return out, attention\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.self_attention = MultiheadAttention(d_model, num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "    self.encoder_decoder_attention = MultiheadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "    self.ffn = PositionwiseFeedForward(d_model = d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "    self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout3 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "  def call(self, x, y , self_attention_mask, cross_attention_mask):\n",
        "    _y = tf.identity(y)\n",
        "    y= self.self_attention(y, mask=self_attention_mask)\n",
        "    y = self.dropout1(y)\n",
        "    y = self.norm1(y + tf.cast(_y, dtype=y.dtype))\n",
        "\n",
        "    _y = tf.identity(y)\n",
        "    y, attention_dist = self.encoder_decoder_attention(x,y,mask=cross_attention_mask)\n",
        "    # print(attention_dist.shape) (30, 8, 200, 200)\n",
        "    y = self.dropout2(y)\n",
        "    y = self.norm2(y + tf.cast(_y, dtype=y.dtype))\n",
        "\n",
        "    _y = tf.identity(y)\n",
        "    y = self.ffn(y)\n",
        "    # print(y.shape) (30, 200, 512)\n",
        "    y = self.dropout3(y)\n",
        "    y = self.norm3(y + tf.cast(_y, dtype=y.dtype))\n",
        "\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "LxrQ9ZeFYS48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _calc_final_dist(x, gens, vocab_dists, attn_dists):\n",
        "        \"\"\"Calculate the final distribution, for the pointer-generator model\n",
        "\n",
        "        Args:\n",
        "          x: encoder input which contain oov number\n",
        "          gens: the generation, choose vocab from article or vocab\n",
        "          vocab_dists: The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays.\n",
        "                       The words are in the order they appear in the vocabulary file.\n",
        "          attn_dists: The attention distributions. List length max_dec_steps of (batch_size, attn_len) arrays\n",
        "\n",
        "        Returns:\n",
        "          final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vsize) arrays.\n",
        "        \"\"\"\n",
        "        # with tf.variable_scope('final_distribution', reuse=tf.AUTO_REUSE):\n",
        "            # Multiply vocab dists by p_gen and attention dists by (1-p_gen)\n",
        "        vocab_dists = gens * vocab_dists\n",
        "        attn_dists = (1-gens) * attn_dists\n",
        "\n",
        "        batch_size = tf.shape(attn_dists)[0]\n",
        "        dec_t = tf.shape(attn_dists)[1]\n",
        "        attn_len = tf.shape(attn_dists)[2]\n",
        "\n",
        "        dec = tf.range(0, limit=dec_t) # [dec]\n",
        "        dec = tf.expand_dims(dec, axis=-1) # [dec, 1]\n",
        "        dec = tf.tile(dec, [1, attn_len]) # [dec, atten_len]\n",
        "        dec = tf.expand_dims(dec, axis=0) # [1, dec, atten_len]\n",
        "        dec = tf.tile(dec, [batch_size, 1, 1]) # [batch_size, dec, atten_len]\n",
        "\n",
        "        x = tf.expand_dims(x, axis=1) # [batch_size, 1, atten_len]\n",
        "        x = tf.tile(x, [1, dec_t, 1]) # [batch_size, dec, atten_len]\n",
        "        x = tf.stack([dec, x], axis=3)\n",
        "\n",
        "        attn_dists_projected = tf.map_fn(fn=lambda y: tf.scatter_nd(y[0], y[1], [dec_t, self.hp.vocab_size]),\n",
        "                                          elems=(x, attn_dists), dtype=tf.float32)\n",
        "\n",
        "        final_dists = attn_dists_projected + vocab_dists\n",
        "\n",
        "        return final_dists"
      ],
      "metadata": {
        "id": "oPFyTf59MKE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x1 = tf.experimental.numpy.random.randn(30, 200, 512)\n",
        "gens = tf.experimental.numpy.random.randn(30, 200)\n",
        "attn_dists = tf.experimental.numpy.random.randn(30, 200, 200)\n",
        "vocab_dists = tf.experimental.numpy.random.randn(30, 200)\n",
        "_calc_final_dist(x, gens, vocab_dists, attn_dists)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "qwKmIKlxMLFK",
        "outputId": "9f79e64f-5fd9-4e9e-c7d2-acd68702a154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-01b036e2a387>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattn_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0m_calc_final_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-82-aa9c0c814c3e>\u001b[0m in \u001b[0;36m_calc_final_dist\u001b[0;34m(x, gens, vocab_dists, attn_dists)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Multiply vocab dists by p_gen and attention dists by (1-p_gen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mvocab_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgens\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvocab_dists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mattn_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattn_dists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5886\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5887\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5888\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [30,200] vs. [30,200,200] [Op:Mul] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = tf.experimental.numpy.random.randn(30, 200, 512)\n",
        "x2 = tf.experimental.numpy.random.randn(30, 200, 200)\n",
        "tf.concat([x1, x2], axis=-1).shape\n",
        "tf.keras.layers.Dense(1,input_shape = tf.concat([x1,x2], axis=-1).shape, activation=tf.sigmoid, use_bias=False)(tf.concat([x1, x2], axis=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnG81yOKHMIc",
        "outputId": "2afcd2a5-0f68-4899-a612-10810e5667f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30, 200, 1), dtype=float32, numpy=\n",
              "array([[[0.22565502],\n",
              "        [0.37905705],\n",
              "        [0.33797178],\n",
              "        ...,\n",
              "        [0.11214606],\n",
              "        [0.3995621 ],\n",
              "        [0.5217021 ]],\n",
              "\n",
              "       [[0.47759828],\n",
              "        [0.46895948],\n",
              "        [0.18123995],\n",
              "        ...,\n",
              "        [0.49253082],\n",
              "        [0.68270475],\n",
              "        [0.18030317]],\n",
              "\n",
              "       [[0.36769792],\n",
              "        [0.13251576],\n",
              "        [0.5622465 ],\n",
              "        ...,\n",
              "        [0.9414817 ],\n",
              "        [0.11035893],\n",
              "        [0.6868868 ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.45803824],\n",
              "        [0.49118292],\n",
              "        [0.41689026],\n",
              "        ...,\n",
              "        [0.4965311 ],\n",
              "        [0.74906635],\n",
              "        [0.30001855]],\n",
              "\n",
              "       [[0.4188706 ],\n",
              "        [0.65569407],\n",
              "        [0.5005605 ],\n",
              "        ...,\n",
              "        [0.39486462],\n",
              "        [0.6766262 ],\n",
              "        [0.7723138 ]],\n",
              "\n",
              "       [[0.8859939 ],\n",
              "        [0.42927206],\n",
              "        [0.29215103],\n",
              "        ...,\n",
              "        [0.9005278 ],\n",
              "        [0.36763868],\n",
              "        [0.06706905]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "    super().__init__()\n",
        "\n",
        "    # self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.layers = [\n",
        "        DecoderLayer(d_model,ffn_hidden,num_heads,drop_prob)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def call(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "    # y = self.sentence_embedding(y, start_token, end_token)\n",
        "    for layer in range(self.num_layers):\n",
        "      y = self.layers[layer](x, y, self_attention_mask, cross_attention_mask)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "G5ytarxiRAqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, [], 'sstart', 'end', 'pad')\n",
        "x = tf.experimental.numpy.random.randn(batch_size, max_sequence_length, d_model)\n",
        "y = tf.experimental.numpy.random.randn(batch_size, max_sequence_length, d_model)\n",
        "mask = np.tril(np.ones( (max_sequence_length,max_sequence_length) ))\n",
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0\n",
        "\n",
        "out = decoder(x,y,mask, None, 'start', 'end')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im0uE-dygdpM",
        "outputId": "34672a6c-25ea-488d-a2c7-ef34aa4251de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 200, 512)\n",
            "(30, 200, 512)\n",
            "(30, 200, 512)\n",
            "(30, 200, 512)\n",
            "(30, 200, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuDYnCAAd4H2",
        "outputId": "5f8cc52f-a1ca-4da8-90a6-2dd455cba19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([30, 200, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "74LYjeqOxHss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceEmbedding(tf.keras.layers.Layer):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            return tf.convert_to_tensor(sentence_word_indicies)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "        tokenized = tf.stack(tokenized)\n",
        "        return tokenized\n",
        "\n",
        "    def call(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder()\n",
        "        x = self.dropout(x + pos)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2Ks2N7GB3E3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, kn_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = tf.convert_to_tensor(np.triu(np.full((max_sequence_length, max_sequence_length), fill_value = True), k=1))\n",
        "    encoder_padding_mask = tf.convert_to_tensor(np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False))\n",
        "    decoder_padding_mask_self_attention = tf.convert_to_tensor(np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False))\n",
        "    decoder_padding_mask_cross_attention = tf.convert_to_tensor(np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False))\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = tf.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  tf.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = tf.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ],
      "metadata": {
        "id": "zH9fpcuzXMQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               d_model,\n",
        "               ffn_hidden,\n",
        "               num_heads,\n",
        "               drop_prob,\n",
        "               num_layers,\n",
        "               max_sequence_length,\n",
        "               kn_vocab_size,\n",
        "               english_to_index,\n",
        "               START_TOKEN,\n",
        "               END_TOKEN,\n",
        "               PADDING_TOKEN):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.linear = tf.keras.layers.Dense(d_model, input_shape=[kn_vocab_size,1], activation=None)\n",
        "\n",
        "    def call(self,\n",
        "                x,\n",
        "                y,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        out = tf.nn.softmax(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "HywSUF73KadG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final"
      ],
      "metadata": {
        "id": "dSDMCckVwnDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 10\n",
        "max_sequence_length = 500\n",
        "ffn_hidden = 2048\n",
        "num_layers = 4"
      ],
      "metadata": {
        "id": "QrVaUBdBwoQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q,k,v,mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = tf.linalg.matmul(q,tf.transpose(k, perm=[0,1,3,2])) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        "  if mask is not None:\n",
        "    scaled = tf.transpose(scaled, perm=[1,0,2,3]) + mask\n",
        "    # scaled = scaled.permute(1, 0, 2, 3) + mask\n",
        "    # scaled = scaled.permute(1, 0, 2, 3)\n",
        "    scaled = tf.transpose(scaled, perm=[1,0,2,3])\n",
        "    # scaled = scaled + mask\n",
        "\n",
        "  attention = tf.nn.softmax(scaled, axis=-1)\n",
        "  out = tf.linalg.matmul(attention,v)\n",
        "  return out, attention\n",
        "\n",
        "class MultiheadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.qkv_layer = tf.keras.layers.Dense(units=3*d_model, input_shape=[d_model,1], activation=None)\n",
        "    # self.qkv_layer = self.add_weight(shape=[input_dim,3*d_model], trainable=True)\n",
        "    self.linear_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "    # self.linear_layer = self.add_weight(shape=[d_model,d_model], trainable=True)\n",
        "\n",
        "  def call(self,x,mask=None):\n",
        "    batch_size, sequence_length, d_model = x.shape\n",
        "    qkv = self.qkv_layer(x)\n",
        "    qkv = tf.reshape(qkv, [batch_size, sequence_length, self.num_heads, 3*self.head_dim])\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q,k,v = tf.split(qkv, 3, axis=-1)\n",
        "    values, attention = scaled_dot_product_attention(q,k,v,mask)\n",
        "    values = tf.reshape(tf.transpose(values, perm=[0, 2, 1, 3]), [batch_size, sequence_length, self.num_heads*self.head_dim])\n",
        "    out = self.linear_layer(values)\n",
        "    return out\n",
        "\n",
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "  def __init__(self, parameters_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters_shape = parameters_shape\n",
        "    self.eps = eps\n",
        "    self.gamma = self.add_weight(shape=parameters_shape, initializer=tf.keras.initializers.Constant(value=1),trainable=True)\n",
        "    self.beta = self.add_weight(shape=parameters_shape, initializer=tf.keras.initializers.Constant(value=0),trainable=True)\n",
        "\n",
        "  def call(self, input):\n",
        "    dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
        "    mean = tf.math.reduce_mean(input, axis = dims, keepdims=True)\n",
        "    var = tf.math.reduce_mean(((input - mean) ** 2), axis = dims, keepdims=True)\n",
        "    std = tf.math.sqrt((var + self.eps))\n",
        "    y = (input - mean) / std\n",
        "    out = self.gamma * y + self.beta\n",
        "    return out\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, max_sequence_length):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "\n",
        "  def call(self):\n",
        "    d_model = tf.cast(self.d_model, tf.float64)\n",
        "    max_sequence_length = tf.cast(self.max_sequence_length, tf.float64)\n",
        "    even_i = tf.experimental.numpy.arange(0, stop=d_model, step=2, dtype=float)\n",
        "    denominator = tf.math.pow(tf.cast(10000, tf.float64), even_i/d_model)\n",
        "    position = tf.reshape(tf.experimental.numpy.arange(0,stop=max_sequence_length, dtype=denominator.dtype), [max_sequence_length,1])\n",
        "    even_PE = tf.math.sin(position/denominator)\n",
        "    odd_PE = tf.math.cos(position/denominator)\n",
        "    stacked = tf.stack([even_PE,odd_PE], axis=2)\n",
        "    PE = tf.keras.layers.Flatten()(stacked)\n",
        "    return PE\n",
        "\n",
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model, hidden, drop_prob):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.linear1 = tf.keras.layers.Dense(units=hidden, input_shape=[d_model,1], activation=None)\n",
        "    self.linear2 = tf.keras.layers.Dense(units=d_model, input_shape=[hidden,1], activation=None)\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "    self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.attention = MultiheadAttention(d_model, num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = tf.keras.layers.Dropout(drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model = d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "  def call(self, x, self_attention_mask):\n",
        "    residual_x = tf.identity(x)\n",
        "    x = self.attention(x, mask=None)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.norm1(x + tf.cast(residual_x, dtype=x.dtype))\n",
        "    residual_x = tf.identity(x)\n",
        "    x = self.ffn(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.norm2(x + tf.cast(residual_x, dtype=x.dtype))\n",
        "    return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, language_to_index, START_TOKEN,END_TOKEN, PADDING_TOKEN):\n",
        "    super().__init__()\n",
        "    self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.layers = [\n",
        "        EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def call(self, x, self_attention_mask, start_token, end_token):\n",
        "    x = self.sentence_embedding(x, start_token, end_token)\n",
        "    for layer in range(self.num_layers):\n",
        "      x = self.layers[layer](x, self_attention_mask)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "yMhAdkM4wpIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadCrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.kv_layer = tf.keras.layers.Dense(units=2*d_model, input_shape=[d_model,1], activation=None)\n",
        "    self.q_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "    self.linear_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "\n",
        "  def call(self, x, y, mask=None):\n",
        "    batch_size, sequence_length, d_model = x.shape\n",
        "    kv = self.kv_layer(x)\n",
        "    q = self.q_layer(x)\n",
        "    kv = tf.reshape(kv, [batch_size, sequence_length, self.num_heads, 2*self.head_dim])\n",
        "    q = tf.reshape(q, [batch_size, sequence_length, self.num_heads, self.head_dim])\n",
        "    kv = tf.transpose(kv, perm=[0, 2, 1, 3])\n",
        "    q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "    k,v = tf.split(kv, 2, axis=-1)\n",
        "    values, attention = scaled_dot_product_attention(q,k,v,mask)\n",
        "    values = tf.reshape(tf.transpose(values, perm=[0, 2, 1, 3]), [batch_size, sequence_length, d_model])\n",
        "    out = self.linear_layer(values)\n",
        "    return out, attention\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.self_attention = MultiheadAttention(d_model, num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "    self.encoder_decoder_attention = MultiheadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "    self.ffn = PositionwiseFeedForward(d_model = d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "    self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout3 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "  def call(self, x, y , self_attention_mask, cross_attention_mask):\n",
        "    _y = tf.identity(y)\n",
        "    y= self.self_attention(y, mask=self_attention_mask)\n",
        "    y = self.dropout1(y)\n",
        "    y = self.norm1(y + tf.cast(_y, dtype=y.dtype))\n",
        "\n",
        "    _y = tf.identity(y)\n",
        "    y, attention_dist = self.encoder_decoder_attention(x,y,mask=cross_attention_mask)\n",
        "    # print(attention_dist.shape) (30, 8, 200, 200)\n",
        "    y = self.dropout2(y)\n",
        "    y = self.norm2(y + tf.cast(_y, dtype=y.dtype))\n",
        "    after_attn_norm = tf.identity(y)\n",
        "\n",
        "    _y = tf.identity(y)\n",
        "    y = self.ffn(y)\n",
        "    # print(y.shape) (30, 200, 512)\n",
        "    y = self.dropout3(y)\n",
        "    y = self.norm3(y + tf.cast(_y, dtype=y.dtype))\n",
        "\n",
        "    return y\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "    super().__init__()\n",
        "\n",
        "    self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.layers = [\n",
        "        DecoderLayer(d_model,ffn_hidden,num_heads,drop_prob)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def call(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "    y = self.sentence_embedding(y, start_token, end_token)\n",
        "    for layer in range(self.num_layers):\n",
        "      y = self.layers[layer](x, y, self_attention_mask, cross_attention_mask)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "PkzRZYw-wx4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceEmbedding(tf.keras.layers.Layer):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = tf.lookup.StaticHashTable(\n",
        "                                      initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "                                      keys=tf.constant(list(language_to_index.keys())),\n",
        "                                      values=tf.constant(list(language_to_index.values())),),\n",
        "                                      default_value=tf.constant(-1),\n",
        "                                      name=\"lang_to_index\")\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "        # tf.config.run_functions_eagerly(True)\n",
        "        # @tf.function\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            input_tensor = tf.constant(list(sentence))\n",
        "            sentence_word_indicies = list(self.language_to_index.lookup(input_tensor))\n",
        "            # sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, self.language_to_index.lookup(tf.constant(self.START_TOKEN)))\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(self.language_to_index.lookup(tf.constant(self.END_TOKEN)))\n",
        "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "                sentence_word_indicies.append(self.language_to_index.lookup(tf.constant(self.PADDING_TOKEN)))\n",
        "            return tf.convert_to_tensor(sentence_word_indicies)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "        tokenized = tf.stack(tokenized)\n",
        "        return tokenized\n",
        "\n",
        "    def call(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder.call()\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n",
        "\n",
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, kn_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = np.triu(np.full((max_sequence_length, max_sequence_length), fill_value = True), k=1)\n",
        "    encoder_padding_mask = np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False)\n",
        "    decoder_padding_mask_self_attention = np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False)\n",
        "    decoder_padding_mask_cross_attention = np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = tf.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  tf.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = tf.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return tf.convert_to_tensor(encoder_self_attention_mask), tf.convert_to_tensor(decoder_self_attention_mask), tf.convert_to_tensor(decoder_cross_attention_mask)"
      ],
      "metadata": {
        "id": "BuMGUixPxBJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Transformer(tf.keras.Model):\n",
        "#   def __init__(self,\n",
        "#                d_model,\n",
        "#                ffn_hidden,\n",
        "#                num_heads,\n",
        "#                drop_prob,\n",
        "#                num_layers,\n",
        "#                max_sequence_length,\n",
        "#                kn_vocab_size,\n",
        "#                english_to_index,\n",
        "#                START_TOKEN,\n",
        "#                END_TOKEN,\n",
        "#                PADDING_TOKEN):\n",
        "#     super(Transformer, self).__init__()\n",
        "\n",
        "#     self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "#     self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "#     self.linear = tf.keras.layers.Dense(d_model, input_shape=[kn_vocab_size,1], activation=None)\n",
        "\n",
        "#     def call(self,\n",
        "#                 x,\n",
        "#                 y,\n",
        "#                 encoder_self_attention_mask=None,\n",
        "#                 decoder_self_attention_mask=None,\n",
        "#                 decoder_cross_attention_mask=None,\n",
        "#                 enc_start_token=False,\n",
        "#                 enc_end_token=False,\n",
        "#                 dec_start_token=False, # We should make this true\n",
        "#                 dec_end_token=False,\n",
        "#                 training = True): # x, y are batch of sentences\n",
        "#         x,y,encoder_self_attention_mask,decoder_self_attention_mask,decoder_cross_attention_mask,enc_start_token,enc_end_token,dec_start_token,dec_end_token,training = inputs[0], inputs[1], inputs[2], inputs[3], inputs[4], inputs[5], inputs[6], inputs[7], inputs[8]\n",
        "#         x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "#         out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "#         out = self.linear(out)\n",
        "#         out = tf.nn.softmax(out)\n",
        "#         return out"
      ],
      "metadata": {
        "id": "q7DTnHUfxM1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               d_model,\n",
        "               ffn_hidden,\n",
        "               num_heads,\n",
        "               drop_prob,\n",
        "               num_layers,\n",
        "               max_sequence_length,\n",
        "               kn_vocab_size,\n",
        "               english_to_index,\n",
        "               START_TOKEN,\n",
        "               END_TOKEN,\n",
        "               PADDING_TOKEN):\n",
        "    super().__init__()\n",
        "    self.kn_vocab_size = kn_vocab_size\n",
        "    self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.linear = tf.keras.layers.Dense(d_model,input_shape=[kn_vocab_size,1], activation=None)\n",
        "    self.embedding = tf.keras.layers.Dense(kn_vocab_size, input_shape=[d_model,1])\n",
        "\n",
        "\n",
        "  def call(self,inputs): # x, y are batch of sentences\n",
        "      x,y,encoder_self_attention_mask,decoder_self_attention_mask,decoder_cross_attention_mask,enc_start_token,enc_end_token,dec_start_token,dec_end_token = inputs[0], inputs[1], inputs[2], inputs[3], inputs[4], inputs[5], inputs[6], inputs[7], inputs[8]\n",
        "      x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "      out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "      out = self.linear(out)\n",
        "      out = self.embedding(out)\n",
        "      out = tf.nn.softmax(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "oQJRdaNxX56r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length,\n",
        "               7,\n",
        "               {'h':0, 'e':1, 'l':2, 'o':3, 'w':4, '<START>':5, '<END>':6, '<PAD>':7},\n",
        "               \"<START>\",\n",
        "               \"<END>\",\n",
        "               \"<PAD>\")"
      ],
      "metadata": {
        "id": "msNd-CP0Tx9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "5I4uPcc_Rmzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks([['hello']], [['how']])\n",
        "pred = transformer(inputs=[['hello'],['how'], encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, None, None, '<START>', '<END>'], training=True)\n",
        "# print(pred.shape)\n",
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQPcev-WUSse",
        "outputId": "a2047279-3761-464c-97f2-ede1eb7b5308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  15766016  \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  21024256  \n",
            "                                                                 \n",
            " dense_55 (Dense)            multiple                  262656    \n",
            "                                                                 \n",
            " dense_56 (Dense)            multiple                  3591      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37056519 (141.36 MB)\n",
            "Trainable params: 37056519 (141.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMm4hvF-dSWP",
        "outputId": "f5974ae6-6e26-47cb-f88d-932f7c2fb43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1000, 7), dtype=float32, numpy=\n",
              "array([[[0.00694466, 0.07771082, 0.10130206, ..., 0.01313011,\n",
              "         0.51431537, 0.04211096],\n",
              "        [0.00480311, 0.1350927 , 0.2116793 , ..., 0.011832  ,\n",
              "         0.2671412 , 0.14882112],\n",
              "        [0.05800103, 0.05924869, 0.06553824, ..., 0.0820801 ,\n",
              "         0.50122434, 0.01989182],\n",
              "        ...,\n",
              "        [0.032977  , 0.02458262, 0.05358369, ..., 0.03552193,\n",
              "         0.66122115, 0.03110315],\n",
              "        [0.02657798, 0.03105725, 0.03105089, ..., 0.04710429,\n",
              "         0.55602485, 0.00764783],\n",
              "        [0.02198996, 0.11633624, 0.02877367, ..., 0.02226381,\n",
              "         0.6122687 , 0.02097907]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = transformer.decoder.sentence_embedding.batch_tokenize(['how'], start_token=False, end_token=True)"
      ],
      "metadata": {
        "id": "ZQibpM0Qdfig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = np.max(pred, axis=-1)\n",
        "temp = tf.cast(temp, labels.dtype)"
      ],
      "metadata": {
        "id": "ncyRRYpTd4i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VNZFpmpqkHw",
        "outputId": "e953b24d-2c68-475f-b235-6c0a00fb7b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1000, 7), dtype=float32, numpy=\n",
              "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 1., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQfyau9q_f36",
        "outputId": "388611d4-cf51-4617-be67-4bd27c6b1a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 1000, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(pred, axis=-1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSnkG6WM-bTA",
        "outputId": "07660587-8290-47a2-abb5-eaed7869a5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.metrics.categorical_crossentropy(np.array([1,2,3,4,5]), np.array([1,2,3,4,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Z6V0D9MUBz0O",
        "outputId": "35ec5579-7802-4b09-bab2-a74a03849709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c28f242eaba7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[1;32m   2197\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2199\u001b[0;31m     \u001b[0mlabel_smoothing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert 0.0 to EagerTensor of dtype int64"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.transpose(pred, perm=[0,2,1]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNJbKkt0A8o4",
        "outputId": "a6d6d2c9-4a0d-459a-c1fa-3b79b63acacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 512, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.decoder.sentence_embedding.embedding(tf.transpose(pred, perm=[0,2,1])).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Ky4cgkAf3c",
        "outputId": "9c837261-cdc1-46bb-e60b-1639152400b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 512, 1000, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.one_hot(transformer.decoder.sentence_embedding.batch_tokenize([['how'],['are'], ['you']], start_token=False, end_token=True),7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugAn1O4V-Aco",
        "outputId": "c11c54db-204a-4739-efb6-1941c18e0ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1000, 7), dtype=float32, numpy=\n",
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 1.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 1.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 1.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Data"
      ],
      "metadata": {
        "id": "UAHO9dDz8p1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterable_dataset = load_dataset(\"csebuetnlp/xlsum\",'english')"
      ],
      "metadata": {
        "id": "lsldPM0C8phc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterable_dataset['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcGlZE9dZYRd",
        "outputId": "2f07e4e5-300d-442f-8ceb-b07bc90a37b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'url', 'title', 'summary', 'text'],\n",
              "    num_rows: 306522\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {' ',\n",
        " '!',\n",
        " '\"',\n",
        " '#',\n",
        " '$',\n",
        " '%',\n",
        " '&',\n",
        " \"'\",\n",
        " '(',\n",
        " ')',\n",
        " '*',\n",
        " '+',\n",
        " ',',\n",
        " '-',\n",
        " '.',\n",
        " '/',\n",
        " '0',\n",
        " '1',\n",
        " '2',\n",
        " '3',\n",
        " '4',\n",
        " '5',\n",
        " '6',\n",
        " '7',\n",
        " '8',\n",
        " '9',\n",
        " ':',\n",
        " ';',\n",
        " '<',\n",
        " '=',\n",
        " '>',\n",
        " '?',\n",
        " '@',\n",
        " 'A',\n",
        " 'B',\n",
        " 'C',\n",
        " 'D',\n",
        " 'E',\n",
        " 'F',\n",
        " 'G',\n",
        " 'H',\n",
        " 'I',\n",
        " 'J',\n",
        " 'K',\n",
        " 'L',\n",
        " 'M',\n",
        " 'N',\n",
        " 'O',\n",
        " 'P',\n",
        " 'Q',\n",
        " 'R',\n",
        " 'S',\n",
        " 'T',\n",
        " 'U',\n",
        " 'V',\n",
        " 'W',\n",
        " 'X',\n",
        " 'Y',\n",
        " 'Z',\n",
        " '[',\n",
        " ']',\n",
        " '^',\n",
        " '_',\n",
        " '`',\n",
        " 'a',\n",
        " 'b',\n",
        " 'c',\n",
        " 'd',\n",
        " 'e',\n",
        " 'f',\n",
        " 'g',\n",
        " 'h',\n",
        " 'i',\n",
        " 'j',\n",
        " 'k',\n",
        " 'l',\n",
        " 'm',\n",
        " 'n',\n",
        " 'o',\n",
        " 'p',\n",
        " 'q',\n",
        " 'r',\n",
        " 's',\n",
        " 't',\n",
        " 'u',\n",
        " 'v',\n",
        " 'w',\n",
        " 'x',\n",
        " 'y',\n",
        " 'z',\n",
        " '{',\n",
        " '|',\n",
        " '}',\n",
        " '~',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " '',\n",
        " ''\n",
        " '\\n'}"
      ],
      "metadata": {
        "id": "C6Mfeiw1oAjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, train_summ = [], []\n",
        "count = 0\n",
        "for i in iterable_dataset['train']:\n",
        "  if len(list(i['text'])) <= max_sequence_length:\n",
        "    if len(set(list(i['text'])) - vocabulary) == 0 and len(set(list(i['summary'])) - vocabulary) == 0:\n",
        "      train_text.append(i['text'])\n",
        "      train_summ.append(i['summary'])\n",
        "\n",
        "  count += 1\n",
        "  if count == 80000:\n",
        "    break"
      ],
      "metadata": {
        "id": "AhIK9jPSEOuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_text), len(train_summ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFUIwjFOF3hu",
        "outputId": "b3825198-ece9-4edd-8ff5-7397c5d10fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6173, 6173)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = np.array(train_text)\n",
        "train_summ = np.array(train_summ)"
      ],
      "metadata": {
        "id": "UYj9LmY4SqjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN\n",
        "final_vocabulary = vocabulary.copy()\n",
        "final_vocabulary.add('<START>')\n",
        "final_vocabulary.add('<END>')\n",
        "final_vocabulary.add('<PAD>')"
      ],
      "metadata": {
        "id": "dimQKKcPJi66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language_to_index = {}\n",
        "\n",
        "for ind, ele in enumerate(final_vocabulary):\n",
        "  language_to_index[ele] = ind"
      ],
      "metadata": {
        "id": "Vk-gbjlqK5PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_TOKEN, END_TOKEN, PADDING_TOKEN = '<START>', '<END>', '<PAD>'"
      ],
      "metadata": {
        "id": "0RfGfx72LNFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(eng_batch, kn_batch,max_sequence_length):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = np.triu(np.full((max_sequence_length, max_sequence_length), fill_value = True), k=1)\n",
        "    encoder_padding_mask = np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False)\n",
        "    decoder_padding_mask_self_attention = np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False)\n",
        "    decoder_padding_mask_cross_attention = np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = tf.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  tf.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = tf.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return tf.convert_to_tensor(encoder_self_attention_mask), tf.convert_to_tensor(decoder_self_attention_mask), tf.convert_to_tensor(decoder_cross_attention_mask)"
      ],
      "metadata": {
        "id": "Osh_eRf1J7Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range_ = list(i for i in range(len(train_text)))\n",
        "def data_generator(batch_size, train_text, train_summ, range_, vocabulary, max_sequence_length):\n",
        "  index = np.random.choice(range_, batch_size, replace=False)\n",
        "  encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(train_text[index], train_summ[index], max_sequence_length)\n",
        "  input = [list(train_text[index]), list(train_summ[index]), encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, None, None, '<START>', '<END>']\n",
        "  y = tf.one_hot(transformer.decoder.sentence_embedding.batch_tokenize(train_summ[index], start_token=False, end_token=True),len(vocabulary))\n",
        "  return input, y\n"
      ],
      "metadata": {
        "id": "MQfwxiv4GNIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class SentenceEmbedding(tf.keras.layers.Layer):\n",
        "#     \"For a given sentence, create an embedding\"\n",
        "#     def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "#         super().__init__()\n",
        "#         self.vocab_size = len(language_to_index)\n",
        "#         self.max_sequence_length = max_sequence_length\n",
        "#         self.embedding = tf.keras.layers.Embedding(self.vocab_size, d_model)\n",
        "#         self.language_to_index = tf.lookup.StaticHashTable(\n",
        "#                                       initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "#                                       keys=tf.constant(list(language_to_index.keys())),\n",
        "#                                       values=tf.constant(list(language_to_index.values())),),\n",
        "#                                       default_value=tf.constant(-1),\n",
        "#                                       name=\"lang_to_index\")\n",
        "#         self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "#         self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
        "#         self.START_TOKEN = START_TOKEN\n",
        "#         self.END_TOKEN = END_TOKEN\n",
        "#         self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "#     def batch_tokenize(self, batch, start_token, end_token):\n",
        "#         # tf.config.run_functions_eagerly(True)\n",
        "#         # @tf.function\n",
        "#         def tokenize(sentence, start_token, end_token):\n",
        "#             input_tensor = tf.constant(list(sentence))\n",
        "#             sentence_word_indicies = list(self.language_to_index.lookup(input_tensor))\n",
        "#             # sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "#             if start_token:\n",
        "#                 sentence_word_indicies.insert(0, self.language_to_index.lookup(tf.constant(self.START_TOKEN)))\n",
        "#             if end_token:\n",
        "#                 sentence_word_indicies.append(self.language_to_index.lookup(tf.constant(self.END_TOKEN)))\n",
        "#             for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "#                 sentence_word_indicies.append(self.language_to_index.lookup(tf.constant(self.PADDING_TOKEN)))\n",
        "#             return tf.convert_to_tensor(sentence_word_indicies)\n",
        "\n",
        "#         tokenized = []\n",
        "#         for sentence_num in range(len(batch)):\n",
        "#            tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "#         tokenized = tf.stack(tokenized)\n",
        "#         return tokenized\n",
        "\n",
        "#     def call(self, x, start_token, end_token): # sentence\n",
        "#         x = self.batch_tokenize(x, start_token, end_token)\n",
        "#         x = self.embedding(x)\n",
        "#         pos = self.position_encoder.call()\n",
        "#         x = self.dropout(x + pos)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "5BudD1yGS9dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length,\n",
        "               len(final_vocabulary),\n",
        "               language_to_index,\n",
        "               \"<START>\",\n",
        "               \"<END>\",\n",
        "               \"<PAD>\")"
      ],
      "metadata": {
        "id": "xYqCUK6JKgnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "aRnl6HukLg67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks([['hello']], [['how']],max_sequence_length)\n",
        "inputs, outputs = data_generator(batch_size, train_text, train_summ, range_, final_vocabulary, max_sequence_length)\n",
        "# [['hello'],['how'], encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, None, None, '<START>', '<END>']\n",
        "pred = transformer(inputs, training=True)\n",
        "# print(pred.shape)\n",
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-kgKCOaLmqC",
        "outputId": "0032a70a-3107-4829-bcec-2e64af9065af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  12667392  \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  16873984  \n",
            "                                                                 \n",
            " dense_44 (Dense)            multiple                  262656    \n",
            "                                                                 \n",
            " dense_45 (Dense)            multiple                  57969     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29862001 (113.91 MB)\n",
            "Trainable params: 29862001 (113.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 10\n",
        "# train_text = np.array(train_text)\n",
        "# train_summ = np.array(train_summ)\n",
        "# range_ = list(i for i in range(len(train_text)))\n",
        "# transformer.fit(data_generator(batch_size, train_text, train_summ, range_, final_vocabulary, max_sequence_length),epochs = 50,steps_per_epoch = 7)"
      ],
      "metadata": {
        "id": "nWwdUg84LpVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "  with tf.GradientTape() as tape:\n",
        "    inputs, outputs = data_generator(batch_size, train_text, train_summ, range_, final_vocabulary, max_sequence_length)\n",
        "    logits = transformer(inputs, training=True)\n",
        "    loss_value = tf.experimental.numpy.sum(tf.keras.metrics.categorical_crossentropy(outputs, logits))\n",
        "  grads = tape.gradient(loss_value, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, transformer.trainable_variables))\n",
        "\n",
        "  if True:\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss_value.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6LLEWlLTDX-",
        "outputId": "ffac3f19-64c4-42c5-f032-59f525dfb8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 18406.873046875\n",
            "Epoch 2/200, Loss: 16424.345703125\n",
            "Epoch 3/200, Loss: 16585.525390625\n",
            "Epoch 4/200, Loss: 16166.455078125\n",
            "Epoch 5/200, Loss: 17617.0859375\n",
            "Epoch 6/200, Loss: 16924.005859375\n",
            "Epoch 7/200, Loss: 18632.525390625\n",
            "Epoch 8/200, Loss: 16069.7470703125\n",
            "Epoch 9/200, Loss: 18052.2734375\n",
            "Epoch 10/200, Loss: 17278.60546875\n",
            "Epoch 11/200, Loss: 17133.54296875\n",
            "Epoch 12/200, Loss: 15827.974609375\n",
            "Epoch 13/200, Loss: 18213.455078125\n",
            "Epoch 14/200, Loss: 18825.94140625\n",
            "Epoch 15/200, Loss: 17133.54296875\n",
            "Epoch 16/200, Loss: 18148.982421875\n",
            "Epoch 17/200, Loss: 18036.15625\n",
            "Epoch 18/200, Loss: 17359.1953125\n",
            "Epoch 19/200, Loss: 18197.3359375\n",
            "Epoch 20/200, Loss: 17326.958984375\n",
            "Epoch 21/200, Loss: 17568.73046875\n",
            "Epoch 22/200, Loss: 14586.880859375\n",
            "Epoch 23/200, Loss: 17149.66015625\n",
            "Epoch 24/200, Loss: 19132.1875\n",
            "Epoch 25/200, Loss: 16988.478515625\n",
            "Epoch 26/200, Loss: 15360.55078125\n",
            "Epoch 27/200, Loss: 17375.3125\n",
            "Epoch 28/200, Loss: 17826.619140625\n",
            "Epoch 29/200, Loss: 16682.234375\n",
            "Epoch 30/200, Loss: 16633.87890625\n",
            "Epoch 31/200, Loss: 16875.65234375\n",
            "Epoch 32/200, Loss: 15876.3291015625\n",
            "Epoch 33/200, Loss: 18358.517578125\n",
            "Epoch 34/200, Loss: 17052.951171875\n",
            "Epoch 35/200, Loss: 16891.76953125\n",
            "Epoch 36/200, Loss: 17891.091796875\n",
            "Epoch 37/200, Loss: 18648.64453125\n",
            "Epoch 38/200, Loss: 17568.73046875\n",
            "Epoch 39/200, Loss: 18745.3515625\n",
            "Epoch 40/200, Loss: 16811.1796875\n",
            "Epoch 41/200, Loss: 17455.90234375\n",
            "Epoch 42/200, Loss: 14796.416015625\n",
            "Epoch 43/200, Loss: 19647.96484375\n",
            "Epoch 44/200, Loss: 17004.59765625\n",
            "Epoch 45/200, Loss: 19599.61328125\n",
            "Epoch 46/200, Loss: 16714.470703125\n",
            "Epoch 47/200, Loss: 17536.494140625\n",
            "Epoch 48/200, Loss: 18020.037109375\n",
            "Epoch 49/200, Loss: 15586.2041015625\n",
            "Epoch 50/200, Loss: 17391.431640625\n",
            "Epoch 51/200, Loss: 17939.4453125\n",
            "Epoch 52/200, Loss: 17375.3125\n",
            "Epoch 53/200, Loss: 16714.46875\n",
            "Epoch 54/200, Loss: 16795.060546875\n",
            "Epoch 55/200, Loss: 15441.140625\n",
            "Epoch 56/200, Loss: 19148.3046875\n",
            "Epoch 57/200, Loss: 16021.392578125\n",
            "Epoch 58/200, Loss: 17423.66796875\n",
            "Epoch 59/200, Loss: 16940.125\n",
            "Epoch 60/200, Loss: 17697.67578125\n",
            "Epoch 61/200, Loss: 17423.66796875\n",
            "Epoch 62/200, Loss: 17536.494140625\n",
            "Epoch 63/200, Loss: 17681.55859375\n",
            "Epoch 64/200, Loss: 16375.990234375\n",
            "Epoch 65/200, Loss: 17423.66796875\n",
            "Epoch 66/200, Loss: 17923.328125\n",
            "Epoch 67/200, Loss: 16714.470703125\n",
            "Epoch 68/200, Loss: 18261.80859375\n",
            "Epoch 69/200, Loss: 18487.4609375\n",
            "Epoch 70/200, Loss: 18294.044921875\n",
            "Epoch 71/200, Loss: 15876.330078125\n",
            "Epoch 72/200, Loss: 16649.998046875\n",
            "Epoch 73/200, Loss: 17198.015625\n",
            "Epoch 74/200, Loss: 15296.078125\n",
            "Epoch 75/200, Loss: 16585.5234375\n",
            "Epoch 76/200, Loss: 16972.359375\n",
            "Epoch 77/200, Loss: 17181.896484375\n",
            "Epoch 78/200, Loss: 19132.1875\n",
            "Epoch 79/200, Loss: 19212.77734375\n",
            "Epoch 80/200, Loss: 18439.109375\n",
            "Epoch 81/200, Loss: 18132.86328125\n",
            "Epoch 82/200, Loss: 17455.90234375\n",
            "Epoch 83/200, Loss: 17923.328125\n",
            "Epoch 84/200, Loss: 17326.958984375\n",
            "Epoch 85/200, Loss: 18519.69921875\n",
            "Epoch 86/200, Loss: 17858.85546875\n",
            "Epoch 87/200, Loss: 17536.494140625\n",
            "Epoch 88/200, Loss: 17101.3046875\n",
            "Epoch 89/200, Loss: 15908.5654296875\n",
            "Epoch 90/200, Loss: 16988.478515625\n",
            "Epoch 91/200, Loss: 17391.431640625\n",
            "Epoch 92/200, Loss: 15682.912109375\n",
            "Epoch 93/200, Loss: 17052.951171875\n",
            "Epoch 94/200, Loss: 17600.96875\n",
            "Epoch 95/200, Loss: 18197.3359375\n",
            "Epoch 96/200, Loss: 18519.69921875\n",
            "Epoch 97/200, Loss: 17794.3828125\n",
            "Epoch 98/200, Loss: 18116.74609375\n",
            "Epoch 99/200, Loss: 17101.3046875\n",
            "Epoch 100/200, Loss: 16537.171875\n",
            "Epoch 101/200, Loss: 18052.2734375\n",
            "Epoch 102/200, Loss: 15940.802734375\n",
            "Epoch 103/200, Loss: 17552.61328125\n",
            "Epoch 104/200, Loss: 16714.470703125\n",
            "Epoch 105/200, Loss: 19293.3671875\n",
            "Epoch 106/200, Loss: 17423.66796875\n",
            "Epoch 107/200, Loss: 18148.982421875\n",
            "Epoch 108/200, Loss: 16633.87890625\n",
            "Epoch 109/200, Loss: 16940.125\n",
            "Epoch 110/200, Loss: 17907.2109375\n",
            "Epoch 111/200, Loss: 17810.5\n",
            "Epoch 112/200, Loss: 17826.619140625\n",
            "Epoch 113/200, Loss: 15553.966796875\n",
            "Epoch 114/200, Loss: 17181.896484375\n",
            "Epoch 115/200, Loss: 20824.587890625\n",
            "Epoch 116/200, Loss: 16504.935546875\n",
            "Epoch 117/200, Loss: 19196.66015625\n",
            "Epoch 118/200, Loss: 19019.359375\n",
            "Epoch 119/200, Loss: 14925.361328125\n",
            "Epoch 120/200, Loss: 19502.90234375\n",
            "Epoch 121/200, Loss: 17455.904296875\n",
            "Epoch 122/200, Loss: 16263.1640625\n",
            "Epoch 123/200, Loss: 15876.3291015625\n",
            "Epoch 124/200, Loss: 16504.93359375\n",
            "Epoch 125/200, Loss: 16569.408203125\n",
            "Epoch 126/200, Loss: 19744.671875\n",
            "Epoch 127/200, Loss: 16988.478515625\n",
            "Epoch 128/200, Loss: 16279.2822265625\n",
            "Epoch 129/200, Loss: 19470.66796875\n",
            "Epoch 130/200, Loss: 17729.91015625\n",
            "Epoch 131/200, Loss: 16311.5185546875\n",
            "Epoch 132/200, Loss: 17826.619140625\n",
            "Epoch 133/200, Loss: 16198.69140625\n",
            "Epoch 134/200, Loss: 18422.990234375\n",
            "Epoch 135/200, Loss: 17101.3046875\n",
            "Epoch 136/200, Loss: 17778.265625\n",
            "Epoch 137/200, Loss: 16698.3515625\n",
            "Epoch 138/200, Loss: 17923.328125\n",
            "Epoch 139/200, Loss: 15795.73828125\n",
            "Epoch 140/200, Loss: 16972.359375\n",
            "Epoch 141/200, Loss: 15602.322265625\n",
            "Epoch 142/200, Loss: 18342.400390625\n",
            "Epoch 143/200, Loss: 18874.296875\n",
            "Epoch 144/200, Loss: 15715.1484375\n",
            "Epoch 145/200, Loss: 17020.71484375\n",
            "Epoch 146/200, Loss: 19986.4453125\n",
            "Epoch 147/200, Loss: 16585.525390625\n",
            "Epoch 148/200, Loss: 17294.72265625\n",
            "Epoch 149/200, Loss: 18906.53125\n",
            "Epoch 150/200, Loss: 18165.1015625\n",
            "Epoch 151/200, Loss: 18358.517578125\n",
            "Epoch 152/200, Loss: 18326.28125\n",
            "Epoch 153/200, Loss: 16811.1796875\n",
            "Epoch 154/200, Loss: 18535.81640625\n",
            "Epoch 155/200, Loss: 17391.43359375\n",
            "Epoch 156/200, Loss: 18068.390625\n",
            "Epoch 157/200, Loss: 18616.40625\n",
            "Epoch 158/200, Loss: 15634.5576171875\n",
            "Epoch 159/200, Loss: 17568.73046875\n",
            "Epoch 160/200, Loss: 17971.68359375\n",
            "Epoch 161/200, Loss: 16537.171875\n",
            "Epoch 162/200, Loss: 17085.1875\n",
            "Epoch 163/200, Loss: 17052.951171875\n",
            "Epoch 164/200, Loss: 16827.296875\n",
            "Epoch 165/200, Loss: 16037.509765625\n",
            "Epoch 166/200, Loss: 16859.533203125\n",
            "Epoch 167/200, Loss: 17681.556640625\n",
            "Epoch 168/200, Loss: 16069.7470703125\n",
            "Epoch 169/200, Loss: 17310.83984375\n",
            "Epoch 170/200, Loss: 17584.849609375\n",
            "Epoch 171/200, Loss: 18310.1640625\n",
            "Epoch 172/200, Loss: 18052.2734375\n",
            "Epoch 173/200, Loss: 15618.439453125\n",
            "Epoch 174/200, Loss: 20050.91796875\n",
            "Epoch 175/200, Loss: 16166.455078125\n",
            "Epoch 176/200, Loss: 14586.880859375\n",
            "Epoch 177/200, Loss: 20502.224609375\n",
            "Epoch 178/200, Loss: 17085.1875\n",
            "Epoch 179/200, Loss: 18068.390625\n",
            "Epoch 180/200, Loss: 18036.15625\n",
            "Epoch 181/200, Loss: 16762.826171875\n",
            "Epoch 182/200, Loss: 17955.564453125\n",
            "Epoch 183/200, Loss: 18181.21875\n",
            "Epoch 184/200, Loss: 17746.029296875\n",
            "Epoch 185/200, Loss: 18551.93359375\n",
            "Epoch 186/200, Loss: 17036.83203125\n",
            "Epoch 187/200, Loss: 18906.533203125\n",
            "Epoch 188/200, Loss: 16166.455078125\n",
            "Epoch 189/200, Loss: 16956.2421875\n",
            "Epoch 190/200, Loss: 17729.91015625\n",
            "Epoch 191/200, Loss: 17600.966796875\n",
            "Epoch 192/200, Loss: 16118.1015625\n",
            "Epoch 193/200, Loss: 16279.2822265625\n",
            "Epoch 194/200, Loss: 16472.69921875\n",
            "Epoch 195/200, Loss: 17617.083984375\n",
            "Epoch 196/200, Loss: 17004.59765625\n",
            "Epoch 197/200, Loss: 19212.77734375\n",
            "Epoch 198/200, Loss: 15827.974609375\n",
            "Epoch 199/200, Loss: 18213.455078125\n",
            "Epoch 200/200, Loss: 18003.91796875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJfRFifiUg_U",
        "outputId": "59b9c4e4-84b8-4b5f-e7ad-6ca0caf1a682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Transformer at 0x7eb450dc21a0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text[112]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "OsAymkaBDZz-",
        "outputId": "6a195062-4012-4567-e219-ef3afbc38067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The officer was taken to hospital where his condition was described as \"stable\", a Merseyside Police spokesman said. He said the force was investigating an \"assault of a member of staff at HMP Altcourse on Wednesday\". The BBC understands the attack took place in a transfer area and the inmate was on remand facing serious charges. Director for HMP Altcourse Steve Williams said: \"We continue to support the officer and his family.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_language = {}\n",
        "\n",
        "for key in language_to_index.keys():\n",
        "  index_to_language[language_to_index[key]] = key"
      ],
      "metadata": {
        "id": "a5S_P53UDrL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kn_sentence = (\"<START>\",)\n",
        "eng_sentence = (train_text[112],)\n",
        "for word_counter in range(max_sequence_length):\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence, max_sequence_length)\n",
        "    predictions = transformer([eng_sentence,\n",
        "                              kn_sentence,\n",
        "                              encoder_self_attention_mask,\n",
        "                              decoder_self_attention_mask,\n",
        "                              decoder_cross_attention_mask,\n",
        "                              False,\n",
        "                              False,\n",
        "                              True,\n",
        "                              False])\n",
        "    next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
        "    next_token_index = np.argmax(next_token_prob_distribution)\n",
        "    next_token = index_to_language[next_token_index]\n",
        "    kn_sentence = (kn_sentence[0] + next_token, )\n",
        "    if next_token == END_TOKEN:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "uIvczIg0DHVT",
        "outputId": "df05ddb1-28fe-4e91-8bd1-3e9e0b0ade5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ed5b81f47dab>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_counter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cross_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkn_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     predictions = transformer([eng_sentence,\n\u001b[0m\u001b[1;32m      6\u001b[0m                               \u001b[0mkn_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f15e297588c6>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# x, y are batch of sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_cross_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_start_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_end_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_start_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_end_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_start_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_end_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cross_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_start_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_end_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-f3ed1bcc56f8>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, self_attention_mask, start_token, end_token)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-f3ed1bcc56f8>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, self_attention_mask)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-f3ed1bcc56f8>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1478\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mpow\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    692\u001b[0m   \"\"\"\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Pow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_pow\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   7281\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7282\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7283\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   7284\u001b[0m         _ctx, \"Pow\", name, x, y)\n\u001b[1;32m   7285\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kn_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3G5QyrfD6Ci",
        "outputId": "8ba16a36-ff02-4234-84e4-13fcde8e9dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<START><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>',)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "54A5TDxWE2tc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
