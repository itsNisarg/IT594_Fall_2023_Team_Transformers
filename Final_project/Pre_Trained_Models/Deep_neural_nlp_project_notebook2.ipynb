{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df3VmMlbxSAX"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q,k,v,mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = tf.linalg.matmul(q,tf.transpose(k, perm=[0,1,3,2])) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "\n",
        "  attention = tf.nn.softmax(scaled, axis=-1)\n",
        "  out = tf.linalg.matmul(attention,v)\n",
        "  return out, attention\n",
        "\n",
        "class MultiheadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.qkv_layer = tf.keras.layers.Dense(units=3*d_model, input_shape=[d_model,1], activation=None)\n",
        "    # self.qkv_layer = self.add_weight(shape=[input_dim,3*d_model], trainable=True)\n",
        "    self.linear_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "    # self.linear_layer = self.add_weight(shape=[d_model,d_model], trainable=True)\n",
        "\n",
        "  def call(self,x,mask=None):\n",
        "    batch_size, sequence_length, d_model = x.shape\n",
        "    qkv = self.qkv_layer(x)\n",
        "    qkv = tf.reshape(qkv, [batch_size, sequence_length, self.num_heads, 3*self.head_dim])\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q,k,v = tf.split(qkv, 3, axis=-1)\n",
        "    values, attention = scaled_dot_product_attention(q,k,v,mask)\n",
        "    values = tf.reshape(tf.transpose(values, perm=[0, 2, 1, 3]), [batch_size, sequence_length, self.num_heads*self.head_dim])\n",
        "    out = self.linear_layer(values)\n",
        "    return out\n",
        "\n",
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "  def __init__(self, parameters_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters_shape = parameters_shape\n",
        "    self.eps = eps\n",
        "    self.gamma = self.add_weight(shape=parameters_shape, initializer=tf.keras.initializers.Constant(value=1),trainable=True)\n",
        "    self.beta = self.add_weight(shape=parameters_shape, initializer=tf.keras.initializers.Constant(value=0),trainable=True)\n",
        "\n",
        "  def call(self, input):\n",
        "    dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
        "    mean = tf.math.reduce_mean(input, axis = dims, keepdims=True)\n",
        "    var = tf.math.reduce_mean(((input - mean) ** 2), axis = dims, keepdims=True)\n",
        "    std = tf.math.sqrt((var + self.eps))\n",
        "    y = (input - mean) / std\n",
        "    out = self.gamma * y + self.beta\n",
        "    return out\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def call(self, d_model, max_sequence_length):\n",
        "    d_model = tf.cast(d_model, tf.float64)\n",
        "    max_sequence_length = tf.cast(max_sequence_length, tf.float64)\n",
        "    even_i = tf.experimental.numpy.arange(0, stop=d_model, step=2, dtype=float)\n",
        "    denominator = tf.math.pow(tf.cast(10000, tf.float64), even_i/d_model)\n",
        "    position = tf.reshape(tf.experimental.numpy.arange(0,stop=max_sequence_length, dtype=denominator.dtype), [max_sequence_length,1])\n",
        "    even_PE = tf.math.sin(position/denominator)\n",
        "    odd_PE = tf.math.cos(position/denominator)\n",
        "    stacked = tf.stack([even_PE,odd_PE], axis=2)\n",
        "    PE = tf.keras.layers.Flatten()(stacked)\n",
        "    return PE\n",
        "\n",
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model, hidden, drop_prob):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.linear1 = tf.keras.layers.Dense(units=hidden, input_shape=[d_model,1], activation=None)\n",
        "    self.linear2 = tf.keras.layers.Dense(units=d_model, input_shape=[hidden,1], activation=None)\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "    self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.attention = MultiheadAttention(d_model, num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = tf.keras.layers.Dropout(drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model = d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "  def call(self, x, self_attention_mask):\n",
        "    residual_x = tf.identity(x)\n",
        "    x = self.attention(x, mask=None)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.norm1(x + tf.cast(residual_x, dtype=x.dtype))\n",
        "    residual_x = tf.identity(x)\n",
        "    x = self.ffn(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.norm2(x + tf.cast(residual_x, dtype=x.dtype))\n",
        "    return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, language_to_index, START_TOKEN,END_TOKEN, PADDING_TOKEN):\n",
        "    super().__init__()\n",
        "    # self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.layers = [\n",
        "        EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def call(self, x, self_attention_mask, start_token, end_token):\n",
        "    # x = self.sentence_embedding(x, start_token, end_token)\n",
        "    for layer in range(self.num_layers):\n",
        "      x = self.layers[layer](x, self_attention_mask)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "SULAVk4JxVLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadCrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.kv_layer = tf.keras.layers.Dense(units=2*d_model, input_shape=[d_model,1], activation=None)\n",
        "    self.q_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "    self.linear_layer = tf.keras.layers.Dense(units=d_model, input_shape=[d_model,1], activation=None)\n",
        "\n",
        "  def call(self, x, y, mask=None):\n",
        "    batch_size, sequence_length, d_model = x.shape\n",
        "    kv = self.kv_layer(x)\n",
        "    q = self.q_layer(x)\n",
        "    kv = tf.reshape(kv, [batch_size, sequence_length, self.num_heads, 2*self.head_dim])\n",
        "    q = tf.reshape(q, [batch_size, sequence_length, self.num_heads, self.head_dim])\n",
        "    kv = tf.transpose(kv, perm=[0, 2, 1, 3])\n",
        "    q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "    k,v = tf.split(kv, 2, axis=-1)\n",
        "    values, attention = scaled_dot_product_attention(q,k,v,mask)\n",
        "    values = tf.reshape(tf.transpose(values, perm=[0, 2, 1, 3]), [batch_size, sequence_length, d_model])\n",
        "    out = self.linear_layer(values)\n",
        "    return out, attention\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.self_attention = MultiheadAttention(d_model, num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "    self.encoder_decoder_attention = MultiheadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "    self.ffn = PositionwiseFeedForward(d_model = d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "    self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout3 = tf.keras.layers.Dropout(drop_prob)\n",
        "\n",
        "  def call(self, x, y , self_attention_mask, cross_attention_mask):\n",
        "    _y = tf.identity(y)\n",
        "    y= self.self_attention(y, mask=self_attention_mask)\n",
        "    y = self.dropout1(y)\n",
        "    y = self.norm1(y + tf.cast(_y, dtype=y.dtype))\n",
        "\n",
        "    _y = tf.identity(y)\n",
        "    y, attention_dist = self.encoder_decoder_attention(x,y,mask=cross_attention_mask)\n",
        "    # print(attention_dist.shape) (30, 8, 200, 200)\n",
        "    y = self.dropout2(y)\n",
        "    y = self.norm2(y + tf.cast(_y, dtype=y.dtype))\n",
        "\n",
        "    _y = tf.identity(y)\n",
        "    y = self.ffn(y)\n",
        "    # print(y.shape) (30, 200, 512)\n",
        "    y = self.dropout3(y)\n",
        "    y = self.norm3(y + tf.cast(_y, dtype=y.dtype))\n",
        "\n",
        "    return y\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "    super().__init__()\n",
        "\n",
        "    # self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.layers = [\n",
        "        DecoderLayer(d_model,ffn_hidden,num_heads,drop_prob)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def call(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "    # y = self.sentence_embedding(y, start_token, end_token)\n",
        "    for layer in range(self.num_layers):\n",
        "      y = self.layers[layer](x, y, self_attention_mask, cross_attention_mask)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "fFCuNp-oxVIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceEmbedding(tf.keras.layers.Layer):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            return tf.convert_to_tensor(sentence_word_indicies)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "        tokenized = tf.stack(tokenized)\n",
        "        return tokenized\n",
        "\n",
        "    def call(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder()\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n",
        "\n",
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, kn_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = tf.convert_to_tensor(np.triu(np.full((max_sequence_length, max_sequence_length), fill_value = True), k=1))\n",
        "    encoder_padding_mask = tf.convert_to_tensor(np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False))\n",
        "    decoder_padding_mask_self_attention = tf.convert_to_tensor(np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False))\n",
        "    decoder_padding_mask_cross_attention = tf.convert_to_tensor(np.full((num_sentences,max_sequence_length, max_sequence_length), fill_value = False))\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = tf.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  tf.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = tf.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ],
      "metadata": {
        "id": "Kiba291rxVFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               d_model,\n",
        "               ffn_hidden,\n",
        "               num_heads,\n",
        "               drop_prob,\n",
        "               num_layers,\n",
        "               max_sequence_length,\n",
        "               kn_vocab_size,\n",
        "               english_to_index,\n",
        "               START_TOKEN,\n",
        "               END_TOKEN,\n",
        "               PADDING_TOKEN):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "    self.linear = tf.keras.layers.Dense(d_model, input_shape=[kn_vocab_size,1], activation=None)\n",
        "\n",
        "    def call(self,\n",
        "                x,\n",
        "                y,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        out = tf.nn.softmax(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Q8z1YP57xbQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}